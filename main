import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Завантаження датасету
file_path = 'IrisData_full.csv'
iris_data = pd.read_csv(file_path)

# Кодування міток (Iris-setosa, Iris-versicolor, Iris-virginica) в числовий формат
le = LabelEncoder()
iris_data.iloc[:, -1] = le.fit_transform(iris_data.iloc[:, -1])

# Приведення міток до типу int
iris_data.iloc[:, -1] = iris_data.iloc[:, -1].astype(int)

# Візуалізація до перемішування
plt.figure(figsize=(14, 8))
plt.subplot(2, 2, 1)
plt.scatter(iris_data.iloc[:, 0], iris_data.iloc[:, 1], c=iris_data.iloc[:, -1], cmap='viridis')
plt.title('Дані до перемішування')
plt.xlabel('Ознака 1')
plt.ylabel('Ознака 2')

# Перемішування датасету
iris_data = iris_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Візуалізація після перемішування
plt.subplot(2, 2, 2)
plt.scatter(iris_data.iloc[:, 0], iris_data.iloc[:, 1], c=iris_data.iloc[:, -1], cmap='viridis')
plt.title('Дані після перемішування')
plt.xlabel('Ознака 1')
plt.ylabel('Ознака 2')

# Виділення ознак і міток (остання колонка - це мітки)
X = iris_data.iloc[:, :-1]  # ознаки
y = iris_data.iloc[:, -1]  # мітки

# Розбиття на тренувальну і тестову вибірки (80% тренувальна, 20% тестова)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Явне приведення цільової змінної до типу int після поділу
y_train = y_train.astype(int)
y_test = y_test.astype(int)

# Візуалізація навчальної і тестової вибірок
plt.subplot(2, 2, 3)
plt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], c=y_train, cmap='viridis', label='Навчальна вибірка')
plt.title('Навчальна вибірка')
plt.xlabel('Ознака 1')
plt.ylabel('Ознака 2')

plt.subplot(2, 2, 4)
plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_test, cmap='viridis', label='Тестова вибірка')
plt.title('Тестова вибірка')
plt.xlabel('Ознака 1')
plt.ylabel('Ознака 2')

plt.tight_layout()
plt.show()

# Нормалізація ознак
scaler = StandardScaler()
X_train_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.transform(X_test)

# Візуалізація нормалізованих даних
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_train_normalized[:, 0], X_train_normalized[:, 1], c=y_train, cmap='viridis')
plt.title('Нормалізовані навчальні дані')
plt.xlabel('Нормалізована ознака 1')
plt.ylabel('Нормалізована ознака 2')

plt.subplot(1, 2, 2)
plt.scatter(X_test_normalized[:, 0], X_test_normalized[:, 1], c=y_test, cmap='viridis')
plt.title('Нормалізовані тестові дані')
plt.xlabel('Нормалізована ознака 1')
plt.ylabel('Нормалізована ознака 2')

plt.tight_layout()
plt.show()

# Тренування та оцінка KNN-класифікатора для різних значень K
accuracies = []
k_values = range(1, 11)
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_normalized, y_train)  # Навчання моделі

    # Прогнозування для тестової вибірки
    y_pred = knn.predict(X_test_normalized)

    # Обчислення точності
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Визначення найкращого значення K
best_k = k_values[np.argmax(accuracies)]
print(f'Найкраще значення K: {best_k}, Точність: {accuracies[np.argmax(accuracies)]}')

# Відображення графіків точності залежно від кількості сусідів
plt.figure(figsize=(12, 6))

# Графік точності
plt.plot(k_values, accuracies, marker='o', color='blue', label='Точність')
plt.xlabel('K (кількість сусідів)')
plt.ylabel('Точність')
plt.title('Залежність точності від кількості сусідів K для KNN-класифікатора')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()
